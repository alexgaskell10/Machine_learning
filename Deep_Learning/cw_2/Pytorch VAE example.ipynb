{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch VAE example.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPNLY7saUKIkexIymLwG9ye"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"M0MbNldzFZZZ","colab_type":"code","outputId":"f0460854-cadb-47e4-b9fb-3f2fca90e47b","executionInfo":{"status":"ok","timestamp":1582280690831,"user_tz":0,"elapsed":364611,"user":{"displayName":"Alex Gaskell","photoUrl":"","userId":"12223185016372309013"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.utils.data\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","\n","\n","# parser = argparse.ArgumentParser(description='VAE MNIST Example')\n","# parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n","#                     help='input batch size for training (default: 128)')\n","# parser.add_argument('--epochs', type=int, default=10, metavar='N',\n","#                     help='number of epochs to train (default: 10)')\n","# parser.add_argument('--no-cuda', action='store_true', default=False,\n","#                     help='enables CUDA training')\n","# parser.add_argument('--seed', type=int, default=1, metavar='S',\n","#                     help='random seed (default: 1)')\n","# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n","#                     help='how many batches to wait before logging training status')\n","# args = parser.parse_args()\n","# args.cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","# torch.manual_seed(args.seed)\n","\n","# device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","\n","# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","batch_size = 32\n","epochs = 20\n","log_interval = 100\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                   transform=transforms.ToTensor()),\n","    batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n","    batch_size=batch_size, shuffle=True)\n","\n","\n","class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","\n","        self.fc1 = nn.Linear(784, 400)\n","        self.fc21 = nn.Linear(400, 20)\n","        self.fc22 = nn.Linear(400, 20)\n","        self.fc3 = nn.Linear(20, 400)\n","        self.fc4 = nn.Linear(400, 784)\n","\n","    def encode(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        return self.fc21(h1), self.fc22(h1)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def decode(self, z):\n","        h3 = F.relu(self.fc3(z))\n","        return torch.sigmoid(self.fc4(h3))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x.view(-1, 784))\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n","\n","\n","model = VAE().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","\n","# Reconstruction + KL divergence losses summed over all elements and batch\n","def loss_function(recon_x, x, mu, logvar):\n","    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n","\n","    # see Appendix B from VAE paper:\n","    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n","    # https://arxiv.org/abs/1312.6114\n","    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","\n","    return BCE + KLD\n","\n","\n","def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, (data, _) in enumerate(train_loader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        recon_batch, mu, logvar = model(data)\n","        loss = loss_function(recon_batch, data, mu, logvar)\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader),\n","                loss.item() / len(data)))\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","          epoch, train_loss / len(train_loader.dataset)))\n","\n","\n","def test(epoch):\n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        for i, (data, _) in enumerate(test_loader):\n","            data = data.to(device)\n","            recon_batch, mu, logvar = model(data)\n","            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n","            if i == 0:\n","                n = min(data.size(0), 8)\n","                comparison = torch.cat([data[:n],\n","                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n","                save_image(comparison.cpu(),\n","                         str(epoch) + '.png', nrow=n)\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('====> Test set loss: {:.4f}'.format(test_loss))\n","\n","if __name__ == \"__main__\":\n","    for epoch in range(1, epochs + 1):\n","        train(epoch)\n","        test(epoch)\n","        with torch.no_grad():\n","            sample = torch.randn(64, 20).to(device)\n","            sample = model.decode(sample).cpu()\n","            save_image(sample.view(64, 1, 28, 28),\n","                       str(epoch) + '.png')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 548.224854\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 202.351257\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 158.185928\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 154.026535\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 139.053955\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 138.240204\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 139.605179\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 128.695236\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 124.515411\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 126.517555\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 130.214233\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 123.816544\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 121.154289\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 126.577347\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 118.659935\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 124.433746\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 122.027191\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 120.745682\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 113.743927\n","====> Epoch: 1 Average loss: 135.8305\n","====> Test set loss: 115.0376\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 119.075821\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 109.362785\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 120.952423\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 123.222939\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 109.146927\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 112.843033\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 114.709587\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 107.467514\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 113.216568\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 119.243515\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 119.115410\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 114.682037\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 113.032364\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 114.359673\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 117.048721\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 114.367653\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 116.866684\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 112.873589\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 113.395576\n","====> Epoch: 2 Average loss: 112.9083\n","====> Test set loss: 109.8881\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 108.363586\n","Train Epoch: 3 [3200/60000 (5%)]\tLoss: 112.702332\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 99.216408\n","Train Epoch: 3 [9600/60000 (16%)]\tLoss: 107.846420\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 108.528847\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 100.782593\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 103.036888\n","Train Epoch: 3 [22400/60000 (37%)]\tLoss: 112.078850\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 113.685555\n","Train Epoch: 3 [28800/60000 (48%)]\tLoss: 112.093018\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 106.379486\n","Train Epoch: 3 [35200/60000 (59%)]\tLoss: 106.677231\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 113.590927\n","Train Epoch: 3 [41600/60000 (69%)]\tLoss: 110.657669\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 107.683243\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 103.849709\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 105.949539\n","Train Epoch: 3 [54400/60000 (91%)]\tLoss: 101.119049\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 105.712624\n","====> Epoch: 3 Average loss: 109.8472\n","====> Test set loss: 108.1295\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 103.456055\n","Train Epoch: 4 [3200/60000 (5%)]\tLoss: 106.688644\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 111.620667\n","Train Epoch: 4 [9600/60000 (16%)]\tLoss: 108.552460\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 110.678185\n","Train Epoch: 4 [16000/60000 (27%)]\tLoss: 105.614395\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 105.152992\n","Train Epoch: 4 [22400/60000 (37%)]\tLoss: 112.102478\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 113.902153\n","Train Epoch: 4 [28800/60000 (48%)]\tLoss: 105.861084\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 112.739807\n","Train Epoch: 4 [35200/60000 (59%)]\tLoss: 106.450676\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 105.906921\n","Train Epoch: 4 [41600/60000 (69%)]\tLoss: 105.076187\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 111.251678\n","Train Epoch: 4 [48000/60000 (80%)]\tLoss: 106.280273\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 109.930618\n","Train Epoch: 4 [54400/60000 (91%)]\tLoss: 107.709892\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 102.726456\n","====> Epoch: 4 Average loss: 108.4427\n","====> Test set loss: 107.4144\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 101.766228\n","Train Epoch: 5 [3200/60000 (5%)]\tLoss: 95.721481\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 109.489319\n","Train Epoch: 5 [9600/60000 (16%)]\tLoss: 108.742996\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 104.004433\n","Train Epoch: 5 [16000/60000 (27%)]\tLoss: 102.768936\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 112.329941\n","Train Epoch: 5 [22400/60000 (37%)]\tLoss: 108.805244\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 108.942619\n","Train Epoch: 5 [28800/60000 (48%)]\tLoss: 110.765244\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 107.350113\n","Train Epoch: 5 [35200/60000 (59%)]\tLoss: 109.294136\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 99.939880\n","Train Epoch: 5 [41600/60000 (69%)]\tLoss: 103.843468\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 112.022316\n","Train Epoch: 5 [48000/60000 (80%)]\tLoss: 105.402306\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 115.193558\n","Train Epoch: 5 [54400/60000 (91%)]\tLoss: 112.310150\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 105.387749\n","====> Epoch: 5 Average loss: 107.5446\n","====> Test set loss: 106.6591\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 109.807480\n","Train Epoch: 6 [3200/60000 (5%)]\tLoss: 113.082260\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 100.435570\n","Train Epoch: 6 [9600/60000 (16%)]\tLoss: 109.668915\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 102.239746\n","Train Epoch: 6 [16000/60000 (27%)]\tLoss: 106.176590\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 102.980927\n","Train Epoch: 6 [22400/60000 (37%)]\tLoss: 102.604584\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 103.471962\n","Train Epoch: 6 [28800/60000 (48%)]\tLoss: 109.497269\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 92.380798\n","Train Epoch: 6 [35200/60000 (59%)]\tLoss: 111.881805\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 111.182907\n","Train Epoch: 6 [41600/60000 (69%)]\tLoss: 97.840088\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 108.117920\n","Train Epoch: 6 [48000/60000 (80%)]\tLoss: 102.239922\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 104.921669\n","Train Epoch: 6 [54400/60000 (91%)]\tLoss: 113.664368\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 102.633896\n","====> Epoch: 6 Average loss: 107.0075\n","====> Test set loss: 106.3340\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 99.470032\n","Train Epoch: 7 [3200/60000 (5%)]\tLoss: 110.780037\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 99.093491\n","Train Epoch: 7 [9600/60000 (16%)]\tLoss: 104.934227\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 106.874710\n","Train Epoch: 7 [16000/60000 (27%)]\tLoss: 113.957870\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 113.765152\n","Train Epoch: 7 [22400/60000 (37%)]\tLoss: 110.663155\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 108.051231\n","Train Epoch: 7 [28800/60000 (48%)]\tLoss: 101.797722\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 106.443962\n","Train Epoch: 7 [35200/60000 (59%)]\tLoss: 111.929329\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 103.129593\n","Train Epoch: 7 [41600/60000 (69%)]\tLoss: 112.890320\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 102.981750\n","Train Epoch: 7 [48000/60000 (80%)]\tLoss: 113.496948\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 99.200943\n","Train Epoch: 7 [54400/60000 (91%)]\tLoss: 103.067642\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 104.592728\n","====> Epoch: 7 Average loss: 106.5103\n","====> Test set loss: 105.8948\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 114.691147\n","Train Epoch: 8 [3200/60000 (5%)]\tLoss: 107.953949\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 112.032394\n","Train Epoch: 8 [9600/60000 (16%)]\tLoss: 103.422157\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 102.717178\n","Train Epoch: 8 [16000/60000 (27%)]\tLoss: 105.794876\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 107.821838\n","Train Epoch: 8 [22400/60000 (37%)]\tLoss: 106.818604\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 106.049805\n","Train Epoch: 8 [28800/60000 (48%)]\tLoss: 99.821625\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 108.781586\n","Train Epoch: 8 [35200/60000 (59%)]\tLoss: 101.644608\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 111.145416\n","Train Epoch: 8 [41600/60000 (69%)]\tLoss: 106.982361\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 108.166718\n","Train Epoch: 8 [48000/60000 (80%)]\tLoss: 106.772263\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 105.651443\n","Train Epoch: 8 [54400/60000 (91%)]\tLoss: 107.723175\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 101.365753\n","====> Epoch: 8 Average loss: 106.1406\n","====> Test set loss: 105.4018\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 111.495560\n","Train Epoch: 9 [3200/60000 (5%)]\tLoss: 99.851624\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 106.321892\n","Train Epoch: 9 [9600/60000 (16%)]\tLoss: 106.770920\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 111.630524\n","Train Epoch: 9 [16000/60000 (27%)]\tLoss: 105.671860\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 110.711121\n","Train Epoch: 9 [22400/60000 (37%)]\tLoss: 99.436371\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 108.625748\n","Train Epoch: 9 [28800/60000 (48%)]\tLoss: 107.697754\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 104.168640\n","Train Epoch: 9 [35200/60000 (59%)]\tLoss: 102.870834\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 105.400391\n","Train Epoch: 9 [41600/60000 (69%)]\tLoss: 107.176834\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 110.600266\n","Train Epoch: 9 [48000/60000 (80%)]\tLoss: 106.000145\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 107.881943\n","Train Epoch: 9 [54400/60000 (91%)]\tLoss: 99.598907\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 109.934303\n","====> Epoch: 9 Average loss: 105.7664\n","====> Test set loss: 105.1934\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 109.448082\n","Train Epoch: 10 [3200/60000 (5%)]\tLoss: 98.878250\n","Train Epoch: 10 [6400/60000 (11%)]\tLoss: 108.400238\n","Train Epoch: 10 [9600/60000 (16%)]\tLoss: 101.151306\n","Train Epoch: 10 [12800/60000 (21%)]\tLoss: 105.298492\n","Train Epoch: 10 [16000/60000 (27%)]\tLoss: 105.970879\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 107.444977\n","Train Epoch: 10 [22400/60000 (37%)]\tLoss: 98.670830\n","Train Epoch: 10 [25600/60000 (43%)]\tLoss: 106.105652\n","Train Epoch: 10 [28800/60000 (48%)]\tLoss: 107.365173\n","Train Epoch: 10 [32000/60000 (53%)]\tLoss: 106.579330\n","Train Epoch: 10 [35200/60000 (59%)]\tLoss: 103.963867\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 100.742996\n","Train Epoch: 10 [41600/60000 (69%)]\tLoss: 100.423286\n","Train Epoch: 10 [44800/60000 (75%)]\tLoss: 108.512955\n","Train Epoch: 10 [48000/60000 (80%)]\tLoss: 108.971558\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 105.671509\n","Train Epoch: 10 [54400/60000 (91%)]\tLoss: 94.576431\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 108.521446\n","====> Epoch: 10 Average loss: 105.5130\n","====> Test set loss: 105.1955\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 103.259010\n","Train Epoch: 11 [3200/60000 (5%)]\tLoss: 106.477982\n","Train Epoch: 11 [6400/60000 (11%)]\tLoss: 100.808273\n","Train Epoch: 11 [9600/60000 (16%)]\tLoss: 105.640182\n","Train Epoch: 11 [12800/60000 (21%)]\tLoss: 104.569702\n","Train Epoch: 11 [16000/60000 (27%)]\tLoss: 103.413918\n","Train Epoch: 11 [19200/60000 (32%)]\tLoss: 105.606766\n","Train Epoch: 11 [22400/60000 (37%)]\tLoss: 101.486267\n","Train Epoch: 11 [25600/60000 (43%)]\tLoss: 106.300781\n","Train Epoch: 11 [28800/60000 (48%)]\tLoss: 111.536880\n","Train Epoch: 11 [32000/60000 (53%)]\tLoss: 100.321091\n","Train Epoch: 11 [35200/60000 (59%)]\tLoss: 102.667526\n","Train Epoch: 11 [38400/60000 (64%)]\tLoss: 102.231827\n","Train Epoch: 11 [41600/60000 (69%)]\tLoss: 100.898392\n","Train Epoch: 11 [44800/60000 (75%)]\tLoss: 109.078720\n","Train Epoch: 11 [48000/60000 (80%)]\tLoss: 96.494896\n","Train Epoch: 11 [51200/60000 (85%)]\tLoss: 112.236115\n","Train Epoch: 11 [54400/60000 (91%)]\tLoss: 104.265396\n","Train Epoch: 11 [57600/60000 (96%)]\tLoss: 103.321236\n","====> Epoch: 11 Average loss: 105.2899\n","====> Test set loss: 104.9914\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 102.965637\n","Train Epoch: 12 [3200/60000 (5%)]\tLoss: 108.980751\n","Train Epoch: 12 [6400/60000 (11%)]\tLoss: 106.954285\n","Train Epoch: 12 [9600/60000 (16%)]\tLoss: 101.955338\n","Train Epoch: 12 [12800/60000 (21%)]\tLoss: 102.067596\n","Train Epoch: 12 [16000/60000 (27%)]\tLoss: 101.114624\n","Train Epoch: 12 [19200/60000 (32%)]\tLoss: 103.381172\n","Train Epoch: 12 [22400/60000 (37%)]\tLoss: 104.994553\n","Train Epoch: 12 [25600/60000 (43%)]\tLoss: 102.180328\n","Train Epoch: 12 [28800/60000 (48%)]\tLoss: 114.237297\n","Train Epoch: 12 [32000/60000 (53%)]\tLoss: 106.677094\n","Train Epoch: 12 [35200/60000 (59%)]\tLoss: 99.678452\n","Train Epoch: 12 [38400/60000 (64%)]\tLoss: 112.048607\n","Train Epoch: 12 [41600/60000 (69%)]\tLoss: 109.054008\n","Train Epoch: 12 [44800/60000 (75%)]\tLoss: 106.461914\n","Train Epoch: 12 [48000/60000 (80%)]\tLoss: 106.929893\n","Train Epoch: 12 [51200/60000 (85%)]\tLoss: 99.448257\n","Train Epoch: 12 [54400/60000 (91%)]\tLoss: 96.694328\n","Train Epoch: 12 [57600/60000 (96%)]\tLoss: 111.360191\n","====> Epoch: 12 Average loss: 105.0751\n","====> Test set loss: 104.7214\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 103.040680\n","Train Epoch: 13 [3200/60000 (5%)]\tLoss: 115.967361\n","Train Epoch: 13 [6400/60000 (11%)]\tLoss: 106.255417\n","Train Epoch: 13 [9600/60000 (16%)]\tLoss: 107.657486\n","Train Epoch: 13 [12800/60000 (21%)]\tLoss: 104.306412\n","Train Epoch: 13 [16000/60000 (27%)]\tLoss: 105.257912\n","Train Epoch: 13 [19200/60000 (32%)]\tLoss: 100.326004\n","Train Epoch: 13 [22400/60000 (37%)]\tLoss: 99.324356\n","Train Epoch: 13 [25600/60000 (43%)]\tLoss: 102.138626\n","Train Epoch: 13 [28800/60000 (48%)]\tLoss: 104.426399\n","Train Epoch: 13 [32000/60000 (53%)]\tLoss: 104.482071\n","Train Epoch: 13 [35200/60000 (59%)]\tLoss: 103.888176\n","Train Epoch: 13 [38400/60000 (64%)]\tLoss: 104.530563\n","Train Epoch: 13 [41600/60000 (69%)]\tLoss: 104.578674\n","Train Epoch: 13 [44800/60000 (75%)]\tLoss: 103.473022\n","Train Epoch: 13 [48000/60000 (80%)]\tLoss: 100.613831\n","Train Epoch: 13 [51200/60000 (85%)]\tLoss: 108.094810\n","Train Epoch: 13 [54400/60000 (91%)]\tLoss: 104.583641\n","Train Epoch: 13 [57600/60000 (96%)]\tLoss: 103.605515\n","====> Epoch: 13 Average loss: 104.8544\n","====> Test set loss: 104.7126\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 100.388908\n","Train Epoch: 14 [3200/60000 (5%)]\tLoss: 105.453865\n","Train Epoch: 14 [6400/60000 (11%)]\tLoss: 96.812477\n","Train Epoch: 14 [9600/60000 (16%)]\tLoss: 104.116547\n","Train Epoch: 14 [12800/60000 (21%)]\tLoss: 108.162842\n","Train Epoch: 14 [16000/60000 (27%)]\tLoss: 107.352646\n","Train Epoch: 14 [19200/60000 (32%)]\tLoss: 105.553436\n","Train Epoch: 14 [22400/60000 (37%)]\tLoss: 104.271935\n","Train Epoch: 14 [25600/60000 (43%)]\tLoss: 100.647644\n","Train Epoch: 14 [28800/60000 (48%)]\tLoss: 96.584930\n","Train Epoch: 14 [32000/60000 (53%)]\tLoss: 96.627312\n","Train Epoch: 14 [35200/60000 (59%)]\tLoss: 110.940308\n","Train Epoch: 14 [38400/60000 (64%)]\tLoss: 107.509674\n","Train Epoch: 14 [41600/60000 (69%)]\tLoss: 104.673782\n","Train Epoch: 14 [44800/60000 (75%)]\tLoss: 107.339035\n","Train Epoch: 14 [48000/60000 (80%)]\tLoss: 103.266937\n","Train Epoch: 14 [51200/60000 (85%)]\tLoss: 109.717567\n","Train Epoch: 14 [54400/60000 (91%)]\tLoss: 95.869789\n","Train Epoch: 14 [57600/60000 (96%)]\tLoss: 93.411293\n","====> Epoch: 14 Average loss: 104.6875\n","====> Test set loss: 104.3511\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 111.360367\n","Train Epoch: 15 [3200/60000 (5%)]\tLoss: 104.590157\n","Train Epoch: 15 [6400/60000 (11%)]\tLoss: 111.245346\n","Train Epoch: 15 [9600/60000 (16%)]\tLoss: 109.663437\n","Train Epoch: 15 [12800/60000 (21%)]\tLoss: 109.485001\n","Train Epoch: 15 [16000/60000 (27%)]\tLoss: 103.723366\n","Train Epoch: 15 [19200/60000 (32%)]\tLoss: 103.374146\n","Train Epoch: 15 [22400/60000 (37%)]\tLoss: 100.505531\n","Train Epoch: 15 [25600/60000 (43%)]\tLoss: 106.339241\n","Train Epoch: 15 [28800/60000 (48%)]\tLoss: 98.598396\n","Train Epoch: 15 [32000/60000 (53%)]\tLoss: 114.823151\n","Train Epoch: 15 [35200/60000 (59%)]\tLoss: 99.788071\n","Train Epoch: 15 [38400/60000 (64%)]\tLoss: 105.128166\n","Train Epoch: 15 [41600/60000 (69%)]\tLoss: 92.633049\n","Train Epoch: 15 [44800/60000 (75%)]\tLoss: 106.055817\n","Train Epoch: 15 [48000/60000 (80%)]\tLoss: 104.374565\n","Train Epoch: 15 [51200/60000 (85%)]\tLoss: 95.079865\n","Train Epoch: 15 [54400/60000 (91%)]\tLoss: 97.844734\n","Train Epoch: 15 [57600/60000 (96%)]\tLoss: 97.811142\n","====> Epoch: 15 Average loss: 104.4412\n","====> Test set loss: 104.5727\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 105.174652\n","Train Epoch: 16 [3200/60000 (5%)]\tLoss: 104.361374\n","Train Epoch: 16 [6400/60000 (11%)]\tLoss: 106.482422\n","Train Epoch: 16 [9600/60000 (16%)]\tLoss: 103.440651\n","Train Epoch: 16 [12800/60000 (21%)]\tLoss: 106.397697\n","Train Epoch: 16 [16000/60000 (27%)]\tLoss: 106.527115\n","Train Epoch: 16 [19200/60000 (32%)]\tLoss: 100.723053\n","Train Epoch: 16 [22400/60000 (37%)]\tLoss: 99.575096\n","Train Epoch: 16 [25600/60000 (43%)]\tLoss: 102.675629\n","Train Epoch: 16 [28800/60000 (48%)]\tLoss: 106.496567\n","Train Epoch: 16 [32000/60000 (53%)]\tLoss: 103.549713\n","Train Epoch: 16 [35200/60000 (59%)]\tLoss: 104.166458\n","Train Epoch: 16 [38400/60000 (64%)]\tLoss: 104.920578\n","Train Epoch: 16 [41600/60000 (69%)]\tLoss: 108.025085\n","Train Epoch: 16 [44800/60000 (75%)]\tLoss: 112.033981\n","Train Epoch: 16 [48000/60000 (80%)]\tLoss: 112.176384\n","Train Epoch: 16 [51200/60000 (85%)]\tLoss: 99.620216\n","Train Epoch: 16 [54400/60000 (91%)]\tLoss: 108.684875\n","Train Epoch: 16 [57600/60000 (96%)]\tLoss: 99.756973\n","====> Epoch: 16 Average loss: 104.3554\n","====> Test set loss: 104.0419\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 106.691963\n","Train Epoch: 17 [3200/60000 (5%)]\tLoss: 98.670258\n","Train Epoch: 17 [6400/60000 (11%)]\tLoss: 109.992828\n","Train Epoch: 17 [9600/60000 (16%)]\tLoss: 109.071152\n","Train Epoch: 17 [12800/60000 (21%)]\tLoss: 114.876312\n","Train Epoch: 17 [16000/60000 (27%)]\tLoss: 97.925842\n","Train Epoch: 17 [19200/60000 (32%)]\tLoss: 106.766556\n","Train Epoch: 17 [22400/60000 (37%)]\tLoss: 108.377464\n","Train Epoch: 17 [25600/60000 (43%)]\tLoss: 104.676323\n","Train Epoch: 17 [28800/60000 (48%)]\tLoss: 101.535011\n","Train Epoch: 17 [32000/60000 (53%)]\tLoss: 105.737000\n","Train Epoch: 17 [35200/60000 (59%)]\tLoss: 97.076447\n","Train Epoch: 17 [38400/60000 (64%)]\tLoss: 104.758652\n","Train Epoch: 17 [41600/60000 (69%)]\tLoss: 109.048630\n","Train Epoch: 17 [44800/60000 (75%)]\tLoss: 101.307228\n","Train Epoch: 17 [48000/60000 (80%)]\tLoss: 110.758484\n","Train Epoch: 17 [51200/60000 (85%)]\tLoss: 104.248657\n","Train Epoch: 17 [54400/60000 (91%)]\tLoss: 99.589813\n","Train Epoch: 17 [57600/60000 (96%)]\tLoss: 104.245743\n","====> Epoch: 17 Average loss: 104.1808\n","====> Test set loss: 104.1633\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 101.539078\n","Train Epoch: 18 [3200/60000 (5%)]\tLoss: 101.947418\n","Train Epoch: 18 [6400/60000 (11%)]\tLoss: 103.181831\n","Train Epoch: 18 [9600/60000 (16%)]\tLoss: 102.989304\n","Train Epoch: 18 [12800/60000 (21%)]\tLoss: 105.033623\n","Train Epoch: 18 [16000/60000 (27%)]\tLoss: 107.051102\n","Train Epoch: 18 [19200/60000 (32%)]\tLoss: 100.216171\n","Train Epoch: 18 [22400/60000 (37%)]\tLoss: 107.305084\n","Train Epoch: 18 [25600/60000 (43%)]\tLoss: 109.040115\n","Train Epoch: 18 [28800/60000 (48%)]\tLoss: 106.980537\n","Train Epoch: 18 [32000/60000 (53%)]\tLoss: 108.823120\n","Train Epoch: 18 [35200/60000 (59%)]\tLoss: 106.715271\n","Train Epoch: 18 [38400/60000 (64%)]\tLoss: 100.192581\n","Train Epoch: 18 [41600/60000 (69%)]\tLoss: 96.738335\n","Train Epoch: 18 [44800/60000 (75%)]\tLoss: 103.176254\n","Train Epoch: 18 [48000/60000 (80%)]\tLoss: 110.465935\n","Train Epoch: 18 [51200/60000 (85%)]\tLoss: 108.579414\n","Train Epoch: 18 [54400/60000 (91%)]\tLoss: 104.155762\n","Train Epoch: 18 [57600/60000 (96%)]\tLoss: 99.574478\n","====> Epoch: 18 Average loss: 104.0693\n","====> Test set loss: 103.8058\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 103.511032\n","Train Epoch: 19 [3200/60000 (5%)]\tLoss: 100.401779\n","Train Epoch: 19 [6400/60000 (11%)]\tLoss: 105.781830\n","Train Epoch: 19 [9600/60000 (16%)]\tLoss: 112.873459\n","Train Epoch: 19 [12800/60000 (21%)]\tLoss: 105.499756\n","Train Epoch: 19 [16000/60000 (27%)]\tLoss: 99.709030\n","Train Epoch: 19 [19200/60000 (32%)]\tLoss: 108.299957\n","Train Epoch: 19 [22400/60000 (37%)]\tLoss: 108.424072\n","Train Epoch: 19 [25600/60000 (43%)]\tLoss: 108.347755\n","Train Epoch: 19 [28800/60000 (48%)]\tLoss: 99.585258\n","Train Epoch: 19 [32000/60000 (53%)]\tLoss: 106.457428\n","Train Epoch: 19 [35200/60000 (59%)]\tLoss: 105.679367\n","Train Epoch: 19 [38400/60000 (64%)]\tLoss: 104.339462\n","Train Epoch: 19 [41600/60000 (69%)]\tLoss: 108.556854\n","Train Epoch: 19 [44800/60000 (75%)]\tLoss: 95.144821\n","Train Epoch: 19 [48000/60000 (80%)]\tLoss: 107.119041\n","Train Epoch: 19 [51200/60000 (85%)]\tLoss: 102.574890\n","Train Epoch: 19 [54400/60000 (91%)]\tLoss: 104.972725\n","Train Epoch: 19 [57600/60000 (96%)]\tLoss: 101.033638\n","====> Epoch: 19 Average loss: 103.9960\n","====> Test set loss: 103.6479\n","Train Epoch: 20 [0/60000 (0%)]\tLoss: 105.913437\n","Train Epoch: 20 [3200/60000 (5%)]\tLoss: 108.132240\n","Train Epoch: 20 [6400/60000 (11%)]\tLoss: 102.399673\n","Train Epoch: 20 [9600/60000 (16%)]\tLoss: 103.827492\n","Train Epoch: 20 [12800/60000 (21%)]\tLoss: 112.050690\n","Train Epoch: 20 [16000/60000 (27%)]\tLoss: 103.209351\n","Train Epoch: 20 [19200/60000 (32%)]\tLoss: 102.540245\n","Train Epoch: 20 [22400/60000 (37%)]\tLoss: 103.447548\n","Train Epoch: 20 [25600/60000 (43%)]\tLoss: 113.193520\n","Train Epoch: 20 [28800/60000 (48%)]\tLoss: 110.308487\n","Train Epoch: 20 [32000/60000 (53%)]\tLoss: 105.521797\n","Train Epoch: 20 [35200/60000 (59%)]\tLoss: 101.323814\n","Train Epoch: 20 [38400/60000 (64%)]\tLoss: 101.170624\n","Train Epoch: 20 [41600/60000 (69%)]\tLoss: 101.571945\n","Train Epoch: 20 [44800/60000 (75%)]\tLoss: 101.839279\n","Train Epoch: 20 [48000/60000 (80%)]\tLoss: 101.062180\n","Train Epoch: 20 [51200/60000 (85%)]\tLoss: 106.246628\n","Train Epoch: 20 [54400/60000 (91%)]\tLoss: 105.544769\n","Train Epoch: 20 [57600/60000 (96%)]\tLoss: 105.664291\n","====> Epoch: 20 Average loss: 103.8486\n","====> Test set loss: 103.6417\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oM7f-j2cHltD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}