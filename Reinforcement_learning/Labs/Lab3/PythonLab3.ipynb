{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (5,5)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,1),(2,1),(2,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [-10, -10, -10, -10, 10] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = 0\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (3,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.8, 0.1, 0.0 , 0.1]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "#         plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    \n",
    "                \n",
    "                        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "The value of that policy is : 0\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val = 0 #Change here!\n",
    "print(\"The value of that policy is : {}\".format(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.zeros(22).astype(int)\n",
    "Policy2[2] = 3\n",
    "Policy2[6] = 2\n",
    "Policy2[18] = 1\n",
    "grid.draw_deterministic_policy(Policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
