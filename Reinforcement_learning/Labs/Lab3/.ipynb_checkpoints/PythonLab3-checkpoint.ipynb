{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (5,5)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,1),(2,1),(2,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [-10, -10, -10, -10, 10] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = 0\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (3,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.8, 0.1, 0.0 , 0.1]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    \n",
    "                \n",
    "                        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "The value of that policy is : 0\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val = 0 #Change here!\n",
    "print(\"The value of that policy is : {}\".format(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'hold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9358c6f60aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPolicy2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPolicy2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_deterministic_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolicy2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-a4be8f73cb69>\u001b[0m in \u001b[0;36mdraw_deterministic_policy\u001b[0;34m(self, Policy)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalls\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewarders\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsorbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsorbing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'hold'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.zeros(22).astype(int)\n",
    "Policy2[2] = 3\n",
    "Policy2[6] = 2\n",
    "Policy2[18] = 1\n",
    "grid.draw_deterministic_policy(Policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
