{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (5,5)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,1),(2,1),(2,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100, -100, -100, 10] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = 0\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (3,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.4, 0.2, 0.2 , 0.2]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)));\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    \n",
    "    \n",
    "    def value_iteration(self, discount = 0.9, threshold = 0.0001):\n",
    "        V = np.zeros(self.state_size)\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        epochs = 0\n",
    "        while True:\n",
    "            epochs+=1\n",
    "            delta = 0\n",
    "\n",
    "            for state_idx in range(self.state_size):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                v = V[state_idx]\n",
    "\n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(self.state_size):\n",
    "                    Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "\n",
    "                V[state_idx]= np.max(Q)\n",
    "                delta = max(delta,np.abs(v - V[state_idx]))\n",
    "            if(delta<threshold):\n",
    "                optimal_policy = np.zeros((self.state_size, self.action_size))\n",
    "                for state_idx in range(self.state_size):\n",
    "                    Q = np.zeros(4)\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    optimal_policy[state_idx, np.argmax(Q)]=1\n",
    "\n",
    "                \n",
    "                return optimal_policy,epochs\n",
    "\n",
    "\n",
    "    \n",
    "    def policy_iteration(self, discount=0.9, threshold = 0.0001):\n",
    "        policy= np.zeros((self.state_size, self.action_size))\n",
    "        policy[:,0] = 1\n",
    "        \n",
    "        T = self.get_transition_matrix()\n",
    "        R = self.get_reward_matrix()\n",
    "        \n",
    "        epochs =0\n",
    "        while True: \n",
    "            V, epochs_eval = self.policy_evaluation(policy, threshold, discount)\n",
    "            \n",
    "            epochs+=epochs_eval\n",
    "            #Policy iteration\n",
    "            policy_stable = True\n",
    "            \n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue \n",
    "                    \n",
    "                old_action = np.argmax(policy[state_idx,:])\n",
    "                \n",
    "                Q = np.zeros(4)\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    Q += T[state_idx_prime,state_idx,:] * (R[state_idx_prime,state_idx, :] + discount * V[state_idx_prime])\n",
    "                \n",
    "                new_policy = np.zeros(4)\n",
    "                new_policy[np.argmax(Q)]=1\n",
    "                policy[state_idx] = new_policy\n",
    "                \n",
    "                if(old_action !=np.argmax(policy[state_idx])):\n",
    "                    policy_stable = False\n",
    "            \n",
    "            if(policy_stable):\n",
    "                return V, policy,epochs\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        epoch = 0\n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            epoch += 1\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V, epoch\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    \n",
    "                \n",
    "                        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "The value of that policy is :[ -3.33640102  -3.67917161  -5.66060949  -4.58932624  -4.02966539\n",
      "  -4.47968254 -11.23257837  -6.12087517  -5.26436677  -7.61656515\n",
      " -26.91149268  -7.98527904 -14.14039494 -52.20116419 -54.55306059\n",
      " -52.2350828  -14.25725978   0.           0.           0.\n",
      "   0.           0.        ]\n",
      "It took 58 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the optimal policy using policy iteration is [ -0.45368571  -0.59964177  -1.22513016  -0.86099204  -0.67773293\n",
      "  -0.56020349  -3.52132441  -1.34216494  -0.87145376  -1.08485296\n",
      " -12.24959854  -1.27275898  -2.73714596 -33.60668986 -36.51183725\n",
      " -33.6267275   -2.78278989   0.           0.           0.\n",
      "   0.           0.        ]:\n",
      "The optimal policy using policy iteration is [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "The number of epochs for convergence are 140\n",
      "The optimal policy using value iteration is [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "The number of epocs for convergence is 53\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "Policy = Policy + 0.25\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "val, epochs = grid.policy_evaluation(Policy,0.001,0.9)\n",
    "print(\"The value of that policy is :{}\".format(val))\n",
    "print(\"It took {} epochs\".format(epochs))\n",
    "\n",
    "\n",
    "gamma_range = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "epochs_needed = []\n",
    "for gamma in gamma_range:\n",
    "    val, epochs = grid.policy_evaluation(Policy,0.001,gamma)\n",
    "    epochs_needed.append(epochs)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(gamma_range,epochs_needed)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "V_opt, pol_opt, epochs = grid.policy_iteration()\n",
    "print(\"The value of the optimal policy using policy iteration is {}:\".format(V_opt))\n",
    "print(\"The optimal policy using policy iteration is {}\".format(pol_opt))\n",
    "print(\"The number of epochs for convergence are {}\".format(epochs))\n",
    "\n",
    "\n",
    "pol_opt2, epochs = grid.value_iteration()\n",
    "print(\"The optimal policy using value iteration is {}\".format(pol_opt2))\n",
    "print(\"The number of epocs for convergence is {}\".format(epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.array([np.argmax(pol_opt[row,:]) for row in range(grid.state_size)])\n",
    "\n",
    "\n",
    "grid.draw_deterministic_policy(Policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
